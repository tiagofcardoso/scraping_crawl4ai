# ğŸš€ AI Web Scraper with OCR, Data Cleaning & RAG System

Um sistema completo de web scraping com OCR, limpeza de dados e sistema RAG (Retrieval-Augmented Generation) para extrair, processar e consultar informaÃ§Ãµes de sites corporativos.

## ğŸ“‹ Funcionalidades

- **ğŸ•·ï¸ Web Scraping AvanÃ§ado**: Screenshots de alta qualidade com Playwright
- **ğŸ” OCR Inteligente**: ExtraÃ§Ã£o de texto com Tesseract e melhoria de imagens
- **ğŸ§¹ Limpeza de Dados**: Processamento automÃ¡tico com NLTK e anÃ¡lise linguÃ­stica
- **ğŸ¤– Sistema RAG**: Busca semÃ¢ntica com FAISS e OpenAI
- **ğŸŒ Interface Web**: Dashboard interativo com Streamlit
- **ğŸ” AutenticaÃ§Ã£o**: Login automÃ¡tico para sites corporativos
- **ğŸŒ Suporte a Proxy**: ConfiguraÃ§Ã£o automÃ¡tica de proxy corporativo

## ğŸ› ï¸ Tecnologias Utilizadas

- **Python 3.8+**
- **Playwright** - AutomaÃ§Ã£o de navegador
- **Tesseract OCR** - ExtraÃ§Ã£o de texto de imagens
- **OpenAI API** - Embeddings e geraÃ§Ã£o de respostas
- **FAISS** - Busca vetorial de similaridade
- **Streamlit** - Interface web interativa
- **NLTK** - Processamento de linguagem natural
- **Pandas** - AnÃ¡lise de dados

## ğŸ“¦ InstalaÃ§Ã£o

### 1ï¸âƒ£ PrÃ©-requisitos

```bash
# Ubuntu/Debian
sudo apt update
sudo apt install python3-pip python3-venv tesseract-ocr tesseract-ocr-por

# Windows (usando Chocolatey)
choco install tesseract

# macOS (usando Homebrew)
brew install tesseract tesseract-lang
```

### 2ï¸âƒ£ ConfiguraÃ§Ã£o do Ambiente

```bash
# Clone o projeto
git clone <seu-repositorio>
cd DevProjects

# Criar ambiente virtual
python3 -m venv .venv
source .venv/bin/activate  # Linux/Mac
# .venv\Scripts\activate   # Windows

# Instalar dependÃªncias
pip install -r requirements.txt

# Instalar navegadores do Playwright
playwright install
```

### 3ï¸âƒ£ ConfiguraÃ§Ã£o de Credenciais

```bash
# Copiar arquivo de exemplo
cp .env.example .env

# Editar com suas credenciais
nano .env
```

**ConteÃºdo do arquivo .env:**

```bash
# OpenAI Configuration
OPENAI_API_KEY=sua_chave_openai_aqui

# Login Credentials
LOGIN_EMAIL=seu.email@empresa.com
LOGIN_PASSWORD=sua_senha_aqui

# Proxy Configuration
USE_PROXY=true
PROXY_PARTNERS_SERVER=http://proxypartners.intratest.com:8080
PROXY_PARTNERS_USERNAME=seu_usuario
PROXY_PARTNERS_PASSWORD=sua_senha_proxy

PROXY_USERS_SERVER=http://proxyusers.intratest.com:8080
PROXY_USERS_USERNAME=seu_usuario
PROXY_USERS_PASSWORD=sua_senha_proxy

# Default Settings
DEFAULT_URL=https://seu-site-corporativo.com
DEFAULT_MAX_DEPTH=2
DEFAULT_CLEAN_DATA=true
DEFAULT_BUILD_RAG=true
DEFAULT_LAUNCH_WEB=true

# Web Interface
WEB_PORT=8501
WEB_HOST=localhost
```

## ğŸš€ Como Executar

### âš¡ ExecuÃ§Ã£o RÃ¡pida (Recomendado)

```bash
python run_scraper_with_cleaning.py
```

Este comando executa todo o pipeline automaticamente:
1. âœ… Verifica dependÃªncias
2. âœ… Executa web scraping com OCR
3. âœ… Limpa e processa os dados
4. âœ… ConstrÃ³i sistema RAG
5. âœ… LanÃ§a interface web interativa

### ğŸ“‹ Ordem de ExecuÃ§Ã£o Detalhada

#### 1ï¸âƒ£ **PREPARAÃ‡ÃƒO (Primeira Vez)**

```bash
# Verificar instalaÃ§Ã£o
python run_scraper_with_cleaning.py
# Escolha "y" para mostrar guia de execuÃ§Ã£o

# Verificar CUDA/GPU (opcional)
# Escolha "y" quando perguntado sobre CUDA
```

#### 2ï¸âƒ£ **PIPELINE COMPLETO**

```bash
python run_scraper_with_cleaning.py
```

**Fluxo interativo:**
- ğŸ“‹ Mostrar guia de execuÃ§Ã£o? (y/N)
- ğŸ” Check CUDA/GPU availability? (y/N)
- ğŸŒ Enter initial URL: `https://seu-site.com`
- ğŸ“Š Enter maximum depth: `2`
- ğŸŒ Use corporate proxy? (Y/n)
- ğŸ§¹ Clean data after scraping? (Y/n)
- ğŸ¤– Build RAG system? (Y/n)
- ğŸŒ Launch web interface? (Y/n)

#### 3ï¸âƒ£ **EXECUÃ‡ÃƒO MODULAR** (Opcional)

```bash
# Apenas Web Scraping
python scraping_crawl4ai.py

# Apenas Limpeza de Dados
python data_cleaner.py

# Apenas Sistema RAG
python rag_system.py

# Apenas Interface Web
streamlit run rag_web_interface.py
```

## ğŸ“‚ Estrutura de Arquivos

```
DevProjects/
â”œâ”€â”€ ğŸ“„ README.md                     # DocumentaÃ§Ã£o
â”œâ”€â”€ ğŸ“„ requirements.txt              # DependÃªncias Python
â”œâ”€â”€ ğŸ“„ .env                         # ConfiguraÃ§Ãµes (criar)
â”œâ”€â”€ ğŸ“„ .env.example                 # Exemplo de configuraÃ§Ãµes
â”‚
â”œâ”€â”€ ğŸ run_scraper_with_cleaning.py # Script principal
â”œâ”€â”€ ğŸ scraping_crawl4ai.py         # MÃ³dulo de scraping
â”œâ”€â”€ ğŸ data_cleaner.py              # MÃ³dulo de limpeza
â”œâ”€â”€ ğŸ rag_system.py                # Sistema RAG
â”œâ”€â”€ ğŸ rag_web_interface.py         # Interface web
â”‚
â””â”€â”€ ğŸ“ scraped_data/                # Dados gerados
    â”œâ”€â”€ ğŸ“ screenshots/             # Screenshots originais
    â”œâ”€â”€ ğŸ“ enhanced/                # Imagens melhoradas
    â”œâ”€â”€ ğŸ“ texts/                   # Textos extraÃ­dos
    â”œâ”€â”€ ğŸ“ cleaned/                 # Dados limpos
    â”œâ”€â”€ ğŸ“ analytics/               # RelatÃ³rios
    â””â”€â”€ ğŸ“ rag_index/               # Ãndice vetorial
```

## ğŸ”§ VerificaÃ§Ã£o do Sistema

### Testar DependÃªncias

```bash
python -c "
import playwright, tesseract, faiss, openai, streamlit
print('âœ… Todas as dependÃªncias instaladas!')
"
```

### Testar CUDA (GPU)

```bash
python -c "
import torch
print(f'CUDA disponÃ­vel: {torch.cuda.is_available()}')
print(f'GPUs: {torch.cuda.device_count()}')
"
```

### Testar OCR

```bash
python -c "
import pytesseract
print('âœ… Tesseract funcionando!')
print(f'VersÃ£o: {pytesseract.get_tesseract_version()}')
"
```

## ğŸ“Š Resultados Esperados

ApÃ³s a execuÃ§Ã£o completa, vocÃª terÃ¡:

### ğŸ“ **scraped_data/texts/**
- Textos extraÃ­dos com metadados
- InformaÃ§Ãµes de proxy, tempo, confianÃ§a OCR

### ğŸ“ **scraped_data/cleaned/**
- Dados limpos em JSON e TXT
- RemoÃ§Ã£o de ruÃ­do e artefatos OCR
- AnÃ¡lise linguÃ­stica e palavras-chave

### ğŸ“ **scraped_data/analytics/**
- RelatÃ³rios de limpeza (CSV/JSON)
- EstatÃ­sticas de eficiÃªncia
- DistribuiÃ§Ã£o de idiomas

### ğŸ“ **scraped_data/rag_index/**
- Ãndice vetorial FAISS
- Embeddings OpenAI
- Sistema de busca semÃ¢ntica

### ğŸŒ **Interface Web** (http://localhost:8501)
- Chat interativo com dados
- Busca por similaridade
- VisualizaÃ§Ã£o de fontes

## ğŸ” Exemplos de Uso

### Consulta via Interface Web

1. Acesse: http://localhost:8501
2. Digite: "Quais sÃ£o os principais produtos da empresa?"
3. Obtenha resposta baseada nos dados coletados

### Consulta via CÃ³digo

```python
from rag_system import RAGSystem

rag = RAGSystem("scraped_data")
result = await rag.query("informaÃ§Ãµes sobre vendas")

print(f"Resposta: {result['answer']}")
print(f"ConfianÃ§a: {result['confidence']}")
print(f"Fontes: {result['sources']}")
```

## ğŸ› SoluÃ§Ã£o de Problemas

### Erro de DependÃªncias

```bash
pip install -r requirements.txt
playwright install
```

### Erro de Tesseract

```bash
# Ubuntu/Debian
sudo apt install tesseract-ocr tesseract-ocr-por

# Verificar instalaÃ§Ã£o
tesseract --version
```

### Erro de Proxy

```bash
# Testar conectividade
curl -x http://usuario:senha@proxy:8080 http://httpbin.org/ip

# Verificar credenciais no .env
```

### Erro OpenAI API

```bash
# Verificar chave no .env
echo $OPENAI_API_KEY

# Testar API
python -c "
import openai
client = openai.OpenAI()
print('âœ… OpenAI API funcionando!')
"
```

## ğŸ“ Logs e Debugging

### Logs Detalhados

```bash
python run_scraper_with_cleaning.py 2>&1 | tee execution.log
```

### Verificar Arquivos Gerados

```bash
# Contar arquivos processados
find scraped_data -name "*.txt" | wc -l
find scraped_data -name "*_cleaned.json" | wc -l

# Ver Ãºltimo arquivo processado
ls -la scraped_data/texts/ | tail -5
```

## ğŸ”’ SeguranÃ§a

- âœ… Credenciais em arquivo .env (nÃ£o versionado)
- âœ… Suporte a proxy corporativo
- âœ… Headers realistas para evitar detecÃ§Ã£o
- âœ… Controle de rate limiting
- âœ… Logs sem credenciais sensÃ­veis

## ğŸ¤ ContribuiÃ§Ã£o

1. Fork o projeto
2. Crie uma branch (`git checkout -b feature/nova-funcionalidade`)
3. Commit suas mudanÃ§as (`git commit -am 'Adiciona nova funcionalidade'`)
4. Push para a branch (`git push origin feature/nova-funcionalidade`)
5. Abra um Pull Request

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ sob a licenÃ§a MIT. Veja o arquivo `LICENSE` para detalhes.

## ğŸ†˜ Suporte

- ğŸ“§ Email: seu.email@empresa.com
- ğŸ“± Teams: @seu_usuario
- ğŸ› Issues: [GitHub Issues](https://github.com/seu-usuario/projeto/issues)

---

**âš¡ InÃ­cio RÃ¡pido:**

```bash
pip install -r requirements.txt
playwright install
cp .env.example .env
# Editar .env com suas credenciais
python run_scraper_with_cleaning.py
```

ğŸ‰ **Pronto! Seu sistema estÃ¡ funcionando!**
