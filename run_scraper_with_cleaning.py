import asyncio
import os
import sys
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

def check_cuda_availability():
    """Check CUDA availability and GPU information"""
    print("\nüîç Checking CUDA and GPU availability...")
    
    # Check PyTorch CUDA
    try:
        import torch
        print(f"‚úÖ PyTorch version: {torch.__version__}")
        
        if torch.cuda.is_available():
            print(f"‚úÖ CUDA is available!")
            print(f"   CUDA version: {torch.version.cuda}")
            print(f"   GPU count: {torch.cuda.device_count()}")
            
            for i in range(torch.cuda.device_count()):
                gpu_name = torch.cuda.get_device_name(i)
                gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3
                print(f"   GPU {i}: {gpu_name} ({gpu_memory:.1f} GB)")
            
            # Test GPU memory allocation
            try:
                device = torch.device('cuda')
                test_tensor = torch.randn(1000, 1000, device=device)
                print(f"‚úÖ GPU memory allocation test: SUCCESS")
                del test_tensor
                torch.cuda.empty_cache()
            except Exception as e:
                print(f"‚ùå GPU memory allocation test failed: {e}")
        else:
            print(f"‚ùå CUDA not available in PyTorch")
            print(f"   Reasons might be:")
            print(f"   - CUDA not installed")
            print(f"   - PyTorch CPU-only version")
            print(f"   - GPU drivers not installed")
    except ImportError:
        print(f"‚ö†Ô∏è  PyTorch not installed")
    
    # Check TensorFlow CUDA (if available)
    try:
        import tensorflow as tf
        print(f"‚úÖ TensorFlow version: {tf.__version__}")
        
        gpus = tf.config.experimental.list_physical_devices('GPU')
        if gpus:
            print(f"‚úÖ TensorFlow GPU devices: {len(gpus)}")
            for i, gpu in enumerate(gpus):
                print(f"   GPU {i}: {gpu}")
        else:
            print(f"‚ùå No GPU devices found by TensorFlow")
    except ImportError:
        print(f"‚ö†Ô∏è  TensorFlow not installed")
    
    # Check NVIDIA System Management Interface
    try:
        import subprocess
        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True, timeout=10)
        if result.returncode == 0:
            print(f"‚úÖ nvidia-smi available:")
            lines = result.stdout.split('\n')
            for line in lines:
                if 'NVIDIA-SMI' in line or 'Driver Version' in line or 'CUDA Version' in line:
                    print(f"   {line.strip()}")
                elif 'GeForce' in line or 'RTX' in line or 'GTX' in line or 'Tesla' in line or 'Quadro' in line:
                    print(f"   {line.strip()}")
        else:
            print(f"‚ùå nvidia-smi not available or failed")
    except (subprocess.TimeoutExpired, FileNotFoundError):
        print(f"‚ùå nvidia-smi command not found")
    except Exception as e:
        print(f"‚ùå Error running nvidia-smi: {e}")
    
    # Check FAISS GPU support
    try:
        import faiss
        print(f"‚úÖ FAISS version: {faiss.__version__ if hasattr(faiss, '__version__') else 'Unknown'}")
        
        # Check if FAISS GPU is available
        try:
            # Try to get GPU resources
            ngpus = faiss.get_num_gpus()
            print(f"‚úÖ FAISS GPU support: {ngpus} GPU(s) available")
            
            if ngpus > 0:
                for i in range(ngpus):
                    res = faiss.StandardGpuResources()
                    print(f"   GPU {i}: Available for FAISS")
        except Exception as e:
            print(f"‚ö†Ô∏è  FAISS GPU support: Not available ({e})")
            print(f"   Using CPU version of FAISS")
    except ImportError:
        print(f"‚ö†Ô∏è  FAISS not installed")
    
    print("\n" + "="*60)

def check_dependencies():
    """Check if all required dependencies are installed"""
    missing_deps = []
    
    try:
        import faiss
        print(f"‚úÖ FAISS available: {faiss.__version__ if hasattr(faiss, '__version__') else 'Unknown'}")
    except ImportError:
        missing_deps.append("faiss-cpu")
    
    try:
        import sentence_transformers
        print(f"‚úÖ Sentence Transformers available")
    except ImportError:
        missing_deps.append("sentence-transformers")
    
    try:
        import streamlit
        print(f"‚úÖ Streamlit available")
    except ImportError:
        missing_deps.append("streamlit")
    
    # Check for additional ML dependencies
    try:
        import torch
        print(f"‚úÖ PyTorch available: {torch.__version__}")
        
        # Check if PyTorch has GPU support
        if torch.cuda.is_available():
            print(f"üéØ GPU acceleration available with your RTX 3050!")
            print(f"   Note: faiss-gpu may not be available via pip for all systems")
            gpu_available = True
        else:
            gpu_available = False
            
    except ImportError:
        print(f"‚ö†Ô∏è  PyTorch not available - some features may be limited")
        missing_deps.append("torch")
        gpu_available = False
    
    if missing_deps:
        print("‚ùå Missing dependencies detected:")
        for dep in missing_deps:
            print(f"   - {dep}")
        print("\nüîß To install missing dependencies, run:")
        print(f"   pip install {' '.join(missing_deps)}")
        print("\nüí° Or install all requirements:")
        print("   pip install -r requirements.txt")
        
        if gpu_available:
            print("\nüöÄ For GPU acceleration (RTX 3050 detected):")
            print("   # Option 1: Try conda for faiss-gpu")
            print("   conda install -c conda-forge faiss-gpu")
            print("   # Option 2: Use faiss-cpu (still fast)")
            print("   pip install faiss-cpu")
            print("   # Note: faiss-cpu will work fine for most workloads!")
        else:
            print("\nüíª Installing CPU versions:")
            print("   pip install faiss-cpu")
        
        return False
    
    return True

def get_env_bool(key, default=False):
    """Convert environment variable to boolean"""
    value = os.getenv(key, str(default)).lower()
    return value in ['true', '1', 'yes', 'on']

def show_execution_guide():
    """Show step-by-step execution guide"""
    print("üìã GUIA DE EXECU√á√ÉO DO PROJETO")
    print("=" * 60)
    print("1Ô∏è‚É£  PREPARAR AMBIENTE:")
    print("   ‚Ä¢ Criar arquivo .env com suas credenciais")
    print("   ‚Ä¢ pip install -r requirements.txt")
    print("   ‚Ä¢ playwright install")
    print()
    print("2Ô∏è‚É£  EXECUTAR PIPELINE COMPLETO:")
    print("   ‚Ä¢ python run_scraper_with_cleaning.py")
    print("   ‚Ä¢ Segue: Scraper ‚Üí Cleaner ‚Üí RAG ‚Üí Web Interface")
    print()
    print("3Ô∏è‚É£  OU EXECUTAR M√ìDULOS INDIVIDUAIS:")
    print("   ‚Ä¢ python scraping_crawl4ai.py (apenas scraping)")
    print("   ‚Ä¢ python data_cleaner.py (apenas limpeza)")
    print("   ‚Ä¢ python rag_system.py (apenas RAG)")
    print()
    print("4Ô∏è‚É£  VERIFICAR RESULTADOS:")
    print("   ‚Ä¢ scraped_data/texts/ (textos extra√≠dos)")
    print("   ‚Ä¢ scraped_data/cleaned/ (dados limpos)")
    print("   ‚Ä¢ scraped_data/analytics/ (relat√≥rios)")
    print("=" * 60)

async def main():
    """Run complete workflow: Scraper -> Cleaner -> RAG"""
    print("üöÄ Complete AI Pipeline: Scraper + Cleaner + RAG")
    print("=" * 60)
    
    # Show execution guide option
    guide_check = input("Show execution guide? (y/N): ").strip().lower()
    if guide_check in ['y', 'yes', 's', 'sim']:
        show_execution_guide()
        if input("\nContinue with execution? (Y/n): ").strip().lower() in ['n', 'no', 'n√£o']:
            return
    
    # Check .env file exists
    if not os.path.exists('.env'):
        print("‚ö†Ô∏è  .env file not found!")
        print("üîß Create a .env file with your credentials:")
        print("   ‚Ä¢ OPENAI_API_KEY=your_key_here")
        print("   ‚Ä¢ LOGIN_EMAIL=your_email@company.com")
        print("   ‚Ä¢ LOGIN_PASSWORD=your_password")
        print("   ‚Ä¢ PROXY_PARTNERS_USERNAME=your_username")
        print("   ‚Ä¢ PROXY_PARTNERS_PASSWORD=your_proxy_password")
        print()
        if input("Continue anyway? (y/N): ").strip().lower() not in ['y', 'yes']:
            return
    
    # Add option to check CUDA
    cuda_check = input("Check CUDA/GPU availability? (y/N): ").strip().lower()
    if cuda_check in ['y', 'yes']:
        check_cuda_availability()
    
    # Check dependencies first
    if not check_dependencies():
        print("\n‚ùå Please install missing dependencies before continuing.")
        print("\nüí° Quick install commands:")
        print("   pip install sentence-transformers streamlit faiss-cpu")
        print("   # This will work great with your RTX 3050!")
        print("\nüîß Alternative with conda (for GPU FAISS):")
        print("   conda install -c conda-forge faiss-gpu sentence-transformers")
        print("   pip install streamlit")
        sys.exit(1)
    
    # Import modules after dependency check
    try:
        from scraping_crawl4ai import ScreenshotScraper
        from data_cleaner import DataCleaner
        from rag_system import RAGSystem
    except ImportError as e:
        print(f"‚ùå Import error: {e}")
        print("üîß Make sure all modules are in the same directory and dependencies are installed.")
        sys.exit(1)
    
    # Get inputs with environment variable defaults
    url = input(f"Enter initial URL (default: {os.getenv('DEFAULT_URL', 'https://example.com')}): ").strip()
    if not url:
        url = os.getenv('DEFAULT_URL', 'https://example.com')
        print(f"Using default URL: {url}")
    
    depth = input(f"Enter maximum depth (default: {os.getenv('DEFAULT_MAX_DEPTH', '2')}): ").strip()
    try:
        max_depth = int(depth) if depth else int(os.getenv('DEFAULT_MAX_DEPTH', '2'))
    except ValueError:
        max_depth = int(os.getenv('DEFAULT_MAX_DEPTH', '2'))
    
    # Use environment variables for proxy configuration
    use_proxy_default = get_env_bool('USE_PROXY', False)
    use_proxy_input = input(f"Use corporate proxy? ({'Y' if use_proxy_default else 'n'}/{'n' if use_proxy_default else 'Y'}): ").strip().lower()
    use_proxy = use_proxy_input not in ['n', 'no'] if use_proxy_input else use_proxy_default
    
    clean_data_default = get_env_bool('DEFAULT_CLEAN_DATA', True)
    clean_data_input = input(f"Clean data after scraping? ({'Y' if clean_data_default else 'n'}/{'n' if clean_data_default else 'Y'}): ").strip().lower()
    clean_data = clean_data_input not in ['n', 'no'] if clean_data_input else clean_data_default
    
    build_rag_default = get_env_bool('DEFAULT_BUILD_RAG', True)
    build_rag_input = input(f"Build RAG system? ({'Y' if build_rag_default else 'n'}/{'n' if build_rag_default else 'Y'}): ").strip().lower()
    build_rag = build_rag_input not in ['n', 'no'] if build_rag_input else build_rag_default
    
    launch_web_default = get_env_bool('DEFAULT_LAUNCH_WEB', True)
    launch_web_input = input(f"Launch web interface? ({'Y' if launch_web_default else 'n'}/{'n' if launch_web_default else 'Y'}): ").strip().lower()
    launch_web = launch_web_input not in ['n', 'no'] if launch_web_input else launch_web_default
    
    # PHASE 1: Web Scraping
    print("\n" + "="*60)
    print("PHASE 1: WEB SCRAPING")
    print("="*60)
    
    # Ask about output format in the main pipeline
    print("üìã Choose capture format:")
    print("1. Screenshot only (OCR only)")
    print("2. PDF only (OCR only)")
    print("3. üöÄ Both (Smart OCR comparison) [RECOMMENDED]")
    
    format_choice = input("Choose format (1/2/3, default: 3): ").strip()
    
    if format_choice == "1":
        output_format = "screenshot"
    elif format_choice == "2":
        output_format = "pdf"
    else:  # Default to "both" (option 3)
        output_format = "both"
    
    print(f"‚úÖ Selected format: {output_format}")
    print("‚ÑπÔ∏è  Note: Only OCR extraction will be used (no HTML parsing)")
    
    scraper = ScreenshotScraper(use_proxy=use_proxy, output_format=output_format)
    await scraper.run(url, max_depth=max_depth)
    
    # PHASE 2: Data Cleaning
    if clean_data:
        print("\n" + "="*60)
        print("PHASE 2: DATA CLEANING")
        print("="*60)
        
        # Check if there are files to clean
        import glob
        text_files = glob.glob(os.path.join(scraper.output_dir, "texts", "*.txt"))
        
        if text_files:
            print(f"üîç Found {len(text_files)} text files to clean")
            cleaner = DataCleaner(scraper.output_dir)
            analytics = await cleaner.clean_all_files()
            
            if analytics:
                await cleaner.export_to_csv()
                print(f"üìà Cleaning efficiency: {analytics['summary']['overall_reduction_percent']:.1f}% reduction")
        else:
            print("‚ö†Ô∏è  No text files found to clean!")
            print("üí° This might be because:")
            print("   ‚Ä¢ Screenshot/PDF capture failed")
            print("   ‚Ä¢ OCR couldn't extract text")
            print("   ‚Ä¢ Network issues prevented page loading")
            print("   ‚Ä¢ Try reducing max_depth or using different URL")

    # PHASE 3: RAG System Building
    if build_rag:
        print("\n" + "="*60)
        print("PHASE 3: RAG SYSTEM BUILDING")
        print("="*60)
        
        print("üîç What's happening in this phase:")
        print("   1. Loading AI embedding model")
        print("   2. Processing cleaned documents")
        print("   3. Creating vector representations")
        print("   4. Building searchable index")
        print()
        
        try:
            rag = RAGSystem(scraper.output_dir)
            rag_success = await rag.build_rag_index()
            
            if rag_success:
                print("‚úÖ RAG system built successfully!")
                print("\nüìä What was created:")
                print("   ‚Ä¢ Vector embeddings for semantic search")
                print("   ‚Ä¢ FAISS index for fast similarity matching")
                print("   ‚Ä¢ Document chunks ready for AI queries")
                print("   ‚Ä¢ Knowledge base ready for Q&A")
                
                # Test query
                test_query = input("\nEnter a test question (or press Enter to skip): ").strip()
                if test_query:
                    print("\nüîç Testing RAG system...")
                    result = await rag.query(test_query)
                    print(f"\nüí° Answer: {result['answer'][:200]}...")
                    print(f"üìä Confidence: {result['confidence']:.2f}")
                    print(f"üìö Sources: {result['documents_found']}")
            else:
                print("‚ùå Failed to build RAG system")
                print("üí° Common fixes:")
                print("   ‚Ä¢ Check if cleaned data exists in scraped_data/cleaned/")
                print("   ‚Ä¢ Ensure OpenAI API key is set in .env")
                print("   ‚Ä¢ Try removing scraped_data/rag/ and rebuilding")
                
        except Exception as e:
            print(f"‚ùå RAG system error: {e}")
            print("üîß Troubleshooting:")
            print("   ‚Ä¢ Remove old index: rm -rf scraped_data/rag/")
            print("   ‚Ä¢ Check file permissions")
            print("   ‚Ä¢ Ensure sufficient disk space")
    
    # PHASE 4: Launch Web Interface
    if launch_web and build_rag:
        print("\n" + "="*60)
        print("PHASE 4: WEB INTERFACE")
        print("="*60)
        
        web_port = os.getenv('WEB_PORT', '8501')
        web_host = os.getenv('WEB_HOST', 'localhost')
        
        print("üåê Launching web interface...")
        print(f"üì± Open your browser and go to: http://{web_host}:{web_port}")
        print("üîÑ Use Ctrl+C to stop the web server")
        
        import subprocess
        import sys
        
        try:
            # Launch Streamlit app with environment variables
            subprocess.run([
                sys.executable, "-m", "streamlit", "run", 
                "rag_web_interface.py", 
                "--server.port", web_port,
                "--server.address", web_host
            ])
        except KeyboardInterrupt:
            print("\nüõë Web interface stopped")
    
    print(f"\nüéâ Complete AI pipeline finished!")
    print(f"üìÇ Check results in: {scraper.output_dir}")
    print(f"ü§ñ RAG system ready for queries!")

if __name__ == "__main__":
    asyncio.run(main())